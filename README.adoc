// INSTRUCTION: Please remove all comments that start INSTRUCTION prior to commit. Most comments should be removed, although not the copyright.
// INSTRUCTION: The copyright statement must appear at the top of the file
//
// Copyright (c) 2017 IBM Corporation and others.
// Licensed under Creative Commons Attribution-NoDerivatives
// 4.0 International (CC BY-ND 4.0)
//   https://creativecommons.org/licenses/by-nd/4.0/
//
// Contributors:
//     IBM Corporation
//
:projectid: istio
:page-layout: guide
:page-duration: 15 minutes
:page-releasedate: 2018-06-30
:page-description: Explore how to route traffic to different versions of the same service using Istio and Kubernetes.
:page-tags: ['microservices', 'Kubernetes', 'Docker', 'containers', 'kubectl', 'istioctl', 'Minikube', 'Istio']
:page-permalink: /guides/{projectid}
:page-related-guides: ['docker', 'kubernetes']
:common-includes: https://raw.githubusercontent.com/OpenLiberty/guides-common/master
:source-highlighter: prettify
:link-to-published-guide: This repository contains the guide documentation source. To view the guide in published form, view it on the https://openliberty.io/guides/{projectid}.html[Open Liberty website].
= Versioning services with Istio

[.hidden]
NOTE: This repository contains the guide documentation source. To view the guide in published form, view it on the https://openliberty.io/guides/{projectid}.html[Open Liberty website].

Explore how to route traffic to different versions of the same service using Istio and Kubernetes.

:kube: Kubernetes
:istio: Istio


== What you'll learn

You will learn how to deploy an application to a Kubernetes cluster using {istio} and how to configure
{istio} to route http requests based on the content of the headers.

The two microservices that you will deploy are called `inventory` and `system`, they are provided for you under
the `start` directory. The `inventory` service stores information about systems that are running the `system` service.
The `system` service provides information about the system that the service is running on, specifically the hostname,
username, and operating system of your computer.

You will use Minikube as your local {kube} cluster. You will use {istio}'s Ingress, and the `istioctl`
cli tool to deploy your application as well as configure the routing. In this guide the `istioctl` tool will be used
to apply routing rules to determine which version of the `inventory` microservice will be accepting requests
based on an `x-version` header.

=== What is {istio}?

{istio} is a platform for managing how microservices interact with each other as well as the outside world.
It works by deploying a few extra services and injecting an additional container into each of your pods that contains
the https://www.envoyproxy.io/[Envoy] proxy. You can think of Envoy as a layer that sits in front of your container
and forwards requests to your container. It is specialized for a service mesh architecture since it offers useful features
such as load balancing and support for distributed tracing.

While {istio} supports {kube} and that will be the focus of this guide, please note that {istio}
can also be used with other environments such as Docker Compose. {istio} has many features, some of them are
request routing, access control, distributed tracing. The focus of this guide will be request routing.

=== Why use {istio}?

{istio} provides a collection of features that allow you to manage several aspects of your services
such as: Routing, Logging, and Security. These features can be easily configured independently of your
application's code.

One use case for {istio} which will be covered in this guide is that it can be used to route between
two different versions of an application. One other reason to use {istio} is that it can be used to enable
https between pods.

// =================================================================================================
// Prerequisites
// =================================================================================================

== Prerequisites

Before you begin, make sure to have the following tools installed:

- `kubernetes-cli` - a command line interface for {kube} called `kubectl`. This is your primary
tool for communicating with and managing your {kube} cluster. Installation instruction can be found here: https://kubernetes.io/docs/tasks/tools/install-kubectl/
- `minikube` - a local, single-node {kube} cluster that runs in a virtual machine. Installation instructions can be found
here: https://github.com/kubernetes/minikube
- `istioctl` - a command line interface for {istio} to deploy {istio} to your {kube} cluster
and to manage your {istio} configurations. Installation instructions can be found here: https://istio.io/docs/setup/kubernetes/quick-start/#download-and-prepare-for-the-installation
- `helm` - a package manager for kubernetes, installation instructions can be found here: https://docs.helm.sh/using_helm/#installing-helm

Since this guide will be using {kube} and Docker, you will also want Docker set up on your computer. You can find installation instructions here: https://docs.docker.com/install/.

// =================================================================================================
// Getting Started
// =================================================================================================

include::{common-includes}/gitclone.adoc[]

// no "try what you'll build" section in this guide since it would be too long due to all setup the user will have to do.

// =================================================================================================
// Staring and preparing your cluster for Deployment
// =================================================================================================

== Starting and preparing your cluster for Deployment

Minikube is a tool that can start and manage a virtual machine on your computer.
The virtual machine is used to run a kubernetes cluster locally.
To begin working with your cluster, first start Minikube with these flags to use {istio}:

Unix:
```
minikube start \
    --extra-config=controller-manager.cluster-signing-cert-file="/var/lib/localkube/certs/ca.crt" \
    --extra-config=controller-manager.cluster-signing-key-file="/var/lib/localkube/certs/ca.key" \
    --extra-config=apiserver.admission-control="NamespaceLifecycle,LimitRanger,ServiceAccount,PersistentVolumeLabel,DefaultStorageClass,DefaultTolerationSeconds,MutatingAdmissionWebhook,ValidatingAdmissionWebhook,ResourceQuota" \
    --kubernetes-version=v1.10.0
```

Windows:
```
minikube start ^
    --bootstrapper=localkube ^
    --vm-driver=hyperv ^
    --hyperv-virtual-switch=minikube-virtual-switch ^
    --extra-config=controller-manager.ClusterSigningCertFile="/var/lib/localkube/certs/ca.crt" ^
    --extra-config=controller-manager.ClusterSigningKeyFile="/var/lib/localkube/certs/ca.key" ^
    --extra-config=apiserver.Admission.PluginNames=NamespaceLifecycle,LimitRanger,ServiceAccount,PersistentVolumeLabel,DefaultStorageClass,DefaultTolerationSeconds,MutatingAdmissionWebhook,ValidatingAdmissionWebhook,ResourceQuota
```

Running the `minikube start ...` command will create and start a virtual machine that will be used
for a local kubernetes cluster. The extra flags are needed for minikube to work correctly
with {istio}.

Connect your Docker CLI to your cluster's Docker Daemon by running the given command.

Unix:
```
eval $(minikube docker-env)
```

Windows:
```
minikube docker-env > tmp.cmd && call tmp.cmd && DEL tmp.cmd
```

As a result of this command, if you issue docker commands they will affect the docker environment
in the Minikube VM rather than the one you likely have installed on your computer. This needs to be the
case because when building the docker containers they need to be available to your cluster since for the purpose
of this guide the images are not being pushed to a docker registry.

First, prepare your {istio} deployment using Helm. Helm is used only for templating, in this situation Helm is not used for
managing the resources as that feature currently not considered stable. Navigate to the directory where you downloaded and installed
{istio} and use the following command to produce an `istio.yaml` file that will be setup to work in the `istio-system` namespace, and have sidecar injection disabled. Enabling sidecar injection allows you to deploy your resources without using the `istio-inject` command later
in the guide, but it'll be done manually in this guide just so that what is happening can be easily observed.
```
helm template install/kubernetes/helm/istio --name istio --namespace istio-system --set sidecarInjectorWebhook.enabled=false > istio.yaml
```

Next, create the `istio-system` namespace because {istio}'s resources need to be deployed to this namespace.
```
kubectl create namespace istio-system
```

You can think of the `istio-system` namespace as a place to keep all of the
{istio} resources in your cluster separate from the rest of your deployments and services.
If you desire, then you can similarly create a namespace for the resources we'll be deploying later.
If no namespace is specified it will use the `default` namespace by default.

Next, deploy {istio}'s resources to your cluster. This will create some services and deployments that are needed by {istio}.
```
kubectl apply -f istio.yaml
```

`kubectl apply ...` creates new resources, or modifies existing ones.

Verify that {istio} was successfully deployed. All the values in the `AVAILABLE` column will have a value of `1` once
the deployment is complete.
```
kubectl get deployments -n istio-system
```
 
Ensure that the istio deployments are all available before continuing, it may take a few minutes for all of them to be available.
[source, role="no_copy"]
----
NAME                       DESIRED   CURRENT   UP-TO-DATE   AVAILABLE   AGE
istio-citadel              1         1         1            1           5m
istio-egressgateway        1         1         1            1           5m
istio-ingress              1         1         1            1           5m
istio-ingressgateway       1         1         1            1           5m
istio-pilot                1         1         1            1           5m
istio-policy               1         1         1            1           5m
istio-statsd-prom-bridge   1         1         1            1           5m
istio-telemetry            1         1         1            1           5m
prometheus                 1         1         1            1           5m
----

// =================================================================================================
// Deploying v1 of the services to cluster
// =================================================================================================

== Deploying version 1 of the services to cluster

Navigate to the `start` directory and run the following command, it may take a few minutes to build.
It will build the application and then package it into two docker images, one for each service.
To build the docker images it uses a maven plugin called `dockerfile-maven-plugin`.
```
mvn clean package
```

After running the command, it builds a docker image for the `inventory` service and an image for the `system` service.
You can verify that these images were created by running the given command. 

```
docker images
```

You will see images called `inventory-service:1.0-SNAPSHOT` and `system-service:1.0-SNAPSHOT` listed in a table
similar to the output below.

[source, role="no_copy"]
----
REPOSITORY                                TAG                 IMAGE ID            CREATED              SIZE
inventory-service                         1.0-SNAPSHOT        b90385c20c52        7 seconds ago        487MB
system-service                            1.0-SNAPSHOT        498fe1db2e75        About a minute ago   487MB
prom/prometheus                           latest              b82ef1f3aa07        2 days ago           119MB
open-liberty                              latest              ba15c5b7d54d        2 weeks ago          487MB
istio/citadel                             0.8.0               3d382d44e0b9        2 weeks ago          51.1MB
istio/mixer                               0.8.0               c5237a3d27ac        2 weeks ago          60.6MB
istio/proxyv2                             0.8.0               a43f156372a7        2 weeks ago          291MB
istio/proxy                               0.8.0               25195d33b0b3        2 weeks ago          174MB
istio/pilot                               0.8.0               8c69f3b2ff64        2 weeks ago          286MB
----

To deploy the `inventory` and `system` services to the {kube} cluster, run the following commands:

Unix:
```
kubectl apply -f <(istioctl kube-inject -f kubernetes.yaml)
```

Windows:
```
istioctl kube-inject -f kubernetes.yaml > tmp.yaml
kubectl apply -f tmp.yaml
DEL tmp.yaml
```

The kubernetes.yaml configuration file contains configuration for various {kube} resources. In this case, it contains
an ingress, two deployments, and two services, which can be broken down as so:

|=========================
| gateway | An Ingress that will route traffic to your services
| inventory-service | Stores information about systems that are running system-service
| inventory-deployment-v1 | Contains the containers that will run version 1 of the inventory application
| system-service | Responds with information about the system that the service is running on
| system-deployment | Contains the containers responsible for system-service
|=========================

By using the command `istioctl kube-inject -f kubernetes.yaml`, it prints a modified version
of the `kubernetes.yaml` file to stdout containing any modifications needed by {istio} such as adding
{istio}'s envoy containers.

You can see that your resources are created after running the `kubectl apply -f ...` command:

[source, role="no_copy"]
----
ingress "gateway" created
service "inventory-service" created
deployment "inventory-deployment-v1" created
service "system-service" created
deployment "system-deployment" created
----

To get the base url, first get the ip address of the cluster.
```
minikube ip
```

Next, find out what port the `istio-ingress` service is running on.
```
kubectl get service istio-ingress -n istio-system
```

You will see output similar to the following:

[source, role="no_copy"]
----
NAME            TYPE           CLUSTER-IP     EXTERNAL-IP   PORT(S)                      AGE
istio-ingress   LoadBalancer   10.109.13.94   <pending>     80:32000/TCP,443:31624/TCP   5m
----

So the port in this specific scenario would be `3200`. To get the base url of the service
combine the port with the ip address you found earlier using the format `http://<ip-address>:<port>`.

Once all the deployments are available, you should be able to navigate to `http://<ip-address>:<port>/inventory/systems` to view
v1 of the deployed application.

You can check that all of the deployments are available using the following command.

```
kubectl get deployments
```

[source, role="no_copy"]
----
NAME                      DESIRED   CURRENT   UP-TO-DATE   AVAILABLE   AGE
inventory-deployment-v1   1         1         1            1           1m
system-deployment         1         1         1            1           1m
----

// =================================================================================================
// Modify the service
// =================================================================================================

== Modify and deploy version 2 of the service

If you make a breaking change to your REST api, existing clients may be broken. This is particularly an issue if your API is public
for anyone to use. Typically you will want a way to keep existing clients working as intended, while new clients use the updated API.
To simulate this scenario, you will introduce a breaking change to the `inventory` service and then redeploy it to the
kubernetes cluster as `v2`.

Rename the `getTotal()` method in `inventory/src/main/java/io/openliberty/guides/inventory/model/InventoryList.java` to `getCount()`.

[source, Java]
----
include::finish/inventory/src/main/java/io/openliberty/guides/inventory/model/InventoryList.java[tags=**;!copyright;]
----

Renaming this method will introduce a breaking change to the REST service because the response will have renamed properties after deployment.
Any client using your service will be expecting to see a property called `total`, but sometimes they'll see `count` instead.
This is not good for your API's consumers since the response would be somewhat unpredictable from their end.

Before version 2 is deployed, the `/inventory/systems` endpoint responds with a JSON in the following form:
```
{
    "systems": [],
    "total": 0
}
```

After version 2 is deployed, the same endpoint can respond with JSON in the following form:
```
{
    "systems": [],
    "count": 0
}
```


Therefore, any clients consuming the updated service will be broken since they are expecting a property called
`total` instead of `count`. Instead of running only the new service, both services can be run simultaneously.
The reason behind running both services is so that old clients do not have to upgrade to use the new api,
they can simply continue to use the old one. For example, if someone has created a frontend for the `inventory` service
and the frontend expects to receive a response in the form of `v1` but gets a response in the form of `v2` then issues can arise.
To mitigate this you can run both versions simultaneously and route requests appropriately to the relevant version, therefore
you avoid breaking existing clients.

Run this command to update the version of your service.
```
mvn versions:set -DnewVersion=2.0-SNAPSHOT
```

Navigate to the `start/inventory` directory and run the given command to rebuild the `inventory` service.
```
mvn clean package
```

Update the `kubernetes.yaml` file to add a new deployment `inventory-deployment-v2`.
Adding a new deployment under the same service with no additional configuration means that it will be load balanced and
requests will be sent to both deployments.


[source, yaml]
----
include::finish/kubernetes.yaml[tags=**]
----

Notice the `version` labels used to denote which deployment is for version 1 of the application and
which deployment is for version 2. This will give us the ability to select which version of the `inventory`
service to forward requests to.


Redeploy your services using the same commands from before.

Unix:
```
kubectl apply -f <(istioctl kube-inject -f kubernetes.yaml)
```

Windows:
```
istioctl kube-inject -f kubernetes.yaml > tmp.yaml
kubectl apply -f tmp.yaml
DEL tmp.yaml
```

You can see that version 2 of the `inventory` application has been deployed.
[source, role="no_copy"]
----
ingress "gateway" unchanged
service "inventory-service" unchanged
deployment "inventory-deployment-v1" configured
deployment "inventory-deployment-v2" created
service "system-service" unchanged
deployment "system-deployment" configured
----

Use the following curl command a few times to make multiple requests `curl http://<ip-address>:<port>/inventory/systems`
Alternatively, you can use https://www.getpostman.com/[Postman] if you prefer a more traditional UI to make requests.

Observe that requests go to both v1 and v2. This is not what
a client consuming this service wants because it is unpredictable and the client would get
an incorrect response half of the time. To solve this you can use request routing to default
to version 1 and allow the client to access version 2 when specifically requested. This way,
existing client won't be broken and any new clients that would like to use v2 can easily
request it.


// =================================================================================================
// Configure request routing
// =================================================================================================

== Configure request routing

To solve the issue where the requests are forwarded seemingly at random, you'll configure request
routing and apply the configuration to your cluster.

Create the file `routing.yaml` in the `start` directory:

[source, yaml]
----
include::finish/routing.yaml[tags=**;]
----

The `routing.yaml` file routes incoming requests based on the uri prefix `/inventory` and an HTTP header
`x-version`. The header is used to identify which version of the `inventory` service that the client would like
the requests to go to. If the header has the value `v2`, then requests will be routed to the `v2` deployment and otherwise
the requests are routed to `v1` as to avoid breaking existing clients. In the `kubernetes.yaml` file some `version` labels are specified,
in the `routing.yaml` file the `version` label is used to route requests to the appropriate
deployment.

Apply the routing rules you created in the routing.yaml file by running the given command.
```
istioctl create -f routing.yaml
```

Make a request and observe that the request goes to v2 of the service where you see "count".
```
curl http://<ip-address>:<port>/inventory/systems -H "x-version: v2"
```

Make a request and observe that the request defaults to v1 of the service where you see "total".
```
curl http://<ip-address>:<port>/inventory/systems
```

With the help of {istio} you now have request routing configured to direct requests appropriately to the correct
version of an application. This is just one of the many uses for {istio}, and it solves the versioning issue
in a convenient way that requires no code changes on the developer's part.

== Testing microservices that are running on {kube}

A few tests are provided to check that the two `inventory` versions have been deployed correctly and that request
routing has been correctly configured. If the tests fail, then you may have made a mistake at one of the steps.

Create the following file `inventory/src/test/java/it/io/openliberty/guides/inventory/InventoryVersionTest.java`:
[source, Java]
----
include::finish/inventory/src/test/java/it/io/openliberty/guides/inventory/InventoryVersionTest.java[tags=**;!copyright;!doc]
----

The `testV1Default` test case verifies that if no `x-version` header is present, then it defaults to version 1.
Similarly the `testV1Header` test case verifies that requests are routed to version if the `x-version` header
has a value of `v1`. The final test case `testV2Header` checks that requests are routed to version 2 when
the `x-version` header is set to `v2`.

Each test is repeated three times to ensure that it didn't pass by chance if the routing was not applied correctly.
If the routing rules are not applied, then on some requests you will see `v1` and on other requests you will see `v2`.
This is why the tests need to be run multiple times.

Run the following command to start the tests:

```
mvn verify -Ddockerfile.skip=true -Dtest.ip=<ip-address> -Dtest.port=<port>
```

The `dockerfile.skip=true` flag skips re-building the docker images. The `test.ip` and `test.port`
parameters refer to the ip address and port for the {istio} gateway.

If the tests pass, then you should see output similar to the following:

[source, role="no_copy"]
----
-------------------------------------------------------
 T E S T S
-------------------------------------------------------
Running it.io.openliberty.guides.inventory.InventoryVersionTest
Tests run: 3, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 2.98 sec - [ ] in it.io.openliberty.guides.inventory.InventoryVersionTest

Results :

Tests run: 3, Failures: 0, Errors: 0, Skipped: 0
----


== Teardown

Finally, you may want to teardown all the deployed resources as a cleanup step.

Delete your resources from the cluster.
```
kubectl delete -f kubernetes.yaml
```

Delete the {istio} resources from the cluster.
```
kubectl delete -f istio.yaml
```

// =================================================================================================
// finish
// =================================================================================================

== Great work! You're done!

You have just deployed two microservice to a Kubernetes cluster and created routing rules to route between two different version of one of the microservice using Istio.

// uncomment this when Istio guide is released
//Feel free to check out our https://github.com/OpenLiberty/guide-istio[Istio guide], which builds on top of what you learned here.

// Include the below from the guides-common repo to tell users how they can contribute to the guide
include::{common-includes}/finish.adoc[]

// DO NO CREATE ANYMORE SECTIONS AT THIS POINT
// Related guides will be added in automatically here if you included them in ":page-related-guides"
